{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6658a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from torchvision.datasets.folder import default_loader, IMG_EXTENSIONS\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import (TensorDataset, \n",
    "                              Dataset, \n",
    "                              Subset,\n",
    "                              random_split,\n",
    "                              DataLoader,\n",
    "                              RandomSampler, \n",
    "                              SequentialSampler, \n",
    "                              )\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from models import initialize_vision_model, initialize_language_model\n",
    "from GarbageUtils import GarbageDataset, split_dataset, GarbageImageFolder\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e57284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "PyTorch Version:  1.13.1+cu117\n",
      "Torchvision Version:  0.14.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(f'Device: {device}')\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae7957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "vision_model_name = \"inception\"\n",
    "language_model_name = \"bert-base-uncased\"\n",
    "num_classes = 4\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "feature_extract = True\n",
    "\n",
    "out_features = 2052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48317b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if language_model_name == \"bert-base-uncased\":\n",
    "    out_features = 2052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475fee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing InceptionV3 with weights=Inception_V3_Weights.DEFAULT...\n",
      "Input size = 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Bert-Base-Uncased...\n"
     ]
    }
   ],
   "source": [
    "vision_model, input_size, vision_out_features = initialize_vision_model(vision_model_name, num_classes, feature_extract, \n",
    "                                                                        use_pretrained=True)\n",
    "language_model, tokenizer = initialize_language_model(language_model_name, num_classes, multimodal=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76513e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): InceptionAux(\n",
       "    (conv0): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): BasicConv2d(\n",
       "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08e0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd091b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = GarbageImageFolder(data_dir, is_valid_file=validate_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff24312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset GarbageImageFolder\n",
       "    Number of datapoints: 5312\n",
       "    Root location: ./data"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8d040c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black', 'blue', 'green', 'other']\n",
      "Num of Classes: 4\n"
     ]
    }
   ],
   "source": [
    "classes =  image_dataset.classes\n",
    "num_classes = len(classes)\n",
    "print(classes)\n",
    "print(f'Num of Classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d92b7c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'black': 0, 'blue': 1, 'green': 2, 'other': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68a38ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<PIL.Image.Image image mode=RGB size=800x800 at 0x1ADB446AAC0>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=800x800 at 0x1ADB446A5B0>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=800x800 at 0x1ADB446A940>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=800x800 at 0x1ADB446AA90>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=800x800 at 0x1ADB446AB20>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=1734x1301 at 0x1ADB446A880>, 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = slice(-3, -1)\n",
    "image_dataset[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9180cfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<PIL.Image.Image image mode=RGB size=1155x1600 at 0x1ADB446A8E0>, 3),\n",
       " (<PIL.Image.Image image mode=RGB size=2615x3044 at 0x1ADB446AC10>, 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3092235b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = split_dataset(image_dataset.imgs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e75ad41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train set size: 3187\n",
      "Val set size: 1062\n",
      "Test set size: 1062\n"
     ]
    }
   ],
   "source": [
    "def get_dataloaders(input_size, train_set, val_set, test_set):\n",
    "    from torchvision import datasets, transforms\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    train_set = GarbageDataset(train_set, is_subset=False, transform=data_transforms['train'])\n",
    "    val_set = GarbageDataset(val_set,  is_subset=False, transform=data_transforms['val'])\n",
    "    test_set = GarbageDataset(test_set,  is_subset=False, transform=data_transforms['val'])\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    print(f'Train set size: {len(train_set)}')\n",
    "    print(f'Val set size: {len(val_set)}')\n",
    "    print(f'Test set size: {len(test_set)}')\n",
    "    \n",
    "    dataloaders_dict = {\n",
    "        'train': DataLoader(train_set, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True),\n",
    "        'val': DataLoader(val_set, batch_size = batch_size, shuffle=False, num_workers=4, drop_last=True)\n",
    "    }\n",
    "    \n",
    "    test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True)\n",
    "    \n",
    "#     print(\"Loading Datasets and Initializing DataLoaders...\")\n",
    "    return test_dataloader, dataloaders_dict\n",
    "\n",
    "test_dataloader, dataloaders_dict = get_dataloaders(input_size, train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f6eb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalGarbageModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes, text_module, vision_module, text_module_name, vision_module_name,\n",
    "                 out_features_combined, dropout_p=None):\n",
    "        super(MultiModalGarbageModel, self).__init__()\n",
    "        self.text_module = text_module\n",
    "        self.vision_module = vision_module\n",
    "        self.text_module_name = text_module_name\n",
    "        self.vision_module_name = vision_module_name\n",
    "        self.fc = torch.nn.Linear(out_features_combined, num_classes)\n",
    "#         self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, vision_data, text_data, attention_mask):\n",
    "        text_out, vision_out = 0, 0\n",
    "        \n",
    "        # get output from vision model\n",
    "        if self.vision_module_name == \"inception\":\n",
    "            self.vision_module.aux_logits = False\n",
    "            vision_out = self.vision_module(vision_data)\n",
    "\n",
    "        # get output from text model    \n",
    "        if self.text_module_name == \"bert-base-uncased\":\n",
    "            out = self.text_module(text_data, attention_mask)\n",
    "            text_out = out[0]\n",
    "        else:\n",
    "            text_out = self.text_module(text_data, attention_mask)\n",
    "            \n",
    "        combined = torch.cat((vision_out, text_out), dim=1)\n",
    "        combined = combined.view(combined.size(0), -1)\n",
    "        combined_out = self.fc(combined)\n",
    "        \n",
    "        return combined_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4cc588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalGarbageModel(num_classes, language_model, vision_model, language_model_name,\n",
    "                                          vision_model_name, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d5986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c661f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "    \n",
    "    val_acc_history = list()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch}/{epochs - 1}')\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            #Iterate over the data # image_file, label, input_ids, attention_mask, file_name\n",
    "            for inputs, labels, in_ids, att_mask in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                in_ids = in_ids.to(device)\n",
    "                att_mask = att_mask.to(device)\n",
    "                \n",
    "#                 all_inputs = {\n",
    "#                     \"input_images\": inputs,\n",
    "#                     \"in_ids\": in_ids,\n",
    "#                     \"att_mask\": att_mask,\n",
    "#                     \"labels\": labels,\n",
    "#                 }\n",
    "                \n",
    "                optimizer.zero_grad() # to zero the parameter gradients\n",
    "                \n",
    "                # forward, track history if only in train mode\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs, in_ids, att_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "#                     if is_inception and phase == \"train\":\n",
    "#                         outputs, aux_outputs = model(inputs)\n",
    "#                         loss1 = criterion(outputs, labels)\n",
    "#                         loss2 = criterion(aux_outputs, labels)\n",
    "#                         loss = loss1 + 0.4*loss2\n",
    "#                     else:\n",
    "#                         outputs = model(inputs)\n",
    "#                         loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # backward, optimize only if in train mode\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss} Accuracy: {epoch_acc}')\n",
    "            \n",
    "            # deepcopy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training Complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Validation Acuuracy: {:.04f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "777bf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, dataloaders, criterion, optimizer, epochs=25, is_inception=False):\n",
    "#     since = time.time()\n",
    "    \n",
    "#     val_acc_history = list()\n",
    "    \n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         print(f'Epoch {epoch}/{epochs - 1}')\n",
    "#         print(\"-\" * 10)\n",
    "        \n",
    "#         for phase in [\"train\", \"val\"]:\n",
    "#             if phase == \"train\":\n",
    "#                 model.train()\n",
    "#             else:\n",
    "#                 model.eval()\n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "            \n",
    "#             #Iterate over the data # image_file, label, input_ids, attention_mask, file_name\n",
    "#             for inputs, labels, _, _ in dataloaders[phase]:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "                \n",
    "#                 optimizer.zero_grad() # to zero the parameter gradients\n",
    "                \n",
    "#                 # forward, track history if only in train mode\n",
    "#                 with torch.set_grad_enabled(phase == \"train\"):\n",
    "#                     if is_inception and phase == \"train\":\n",
    "#                         outputs, aux_outputs = model(inputs)\n",
    "#                         loss1 = criterion(outputs, labels)\n",
    "#                         loss2 = criterion(aux_outputs, labels)\n",
    "#                         loss = loss1 + 0.4*loss2\n",
    "#                     else:\n",
    "#                         outputs = model(inputs)\n",
    "#                         loss = criterion(outputs, labels)\n",
    "                    \n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "#                     # backward, optimize only if in train mode\n",
    "#                     if phase == \"train\":\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "                        \n",
    "#                 # statistics\n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "#             epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "#             print(f'{phase} Loss: {epoch_loss} Accuracy: {epoch_acc}')\n",
    "            \n",
    "#             # deepcopy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#             if phase == 'val':\n",
    "#                 val_acc_history.append(epoch_acc)\n",
    "#         print()\n",
    "        \n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training Complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best Validation Acuuracy: {:.04f}'.format(best_acc))\n",
    "    \n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    \n",
    "#     return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87699025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to learn:\n",
      "\t text_module.bert.embeddings.word_embeddings.weight\n",
      "\t text_module.bert.embeddings.position_embeddings.weight\n",
      "\t text_module.bert.embeddings.token_type_embeddings.weight\n",
      "\t text_module.bert.embeddings.LayerNorm.weight\n",
      "\t text_module.bert.embeddings.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.0.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.0.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.1.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.1.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.2.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.2.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.3.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.3.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.4.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.4.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.5.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.5.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.6.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.6.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.7.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.7.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.8.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.8.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.9.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.9.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.10.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.10.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.11.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.11.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\t text_module.bert.pooler.dense.weight\n",
      "\t text_module.bert.pooler.dense.bias\n",
      "\t text_module.classifier.weight\n",
      "\t text_module.classifier.bias\n",
      "\t vision_module.AuxLogits.fc.weight\n",
      "\t vision_module.AuxLogits.fc.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# send model to device: gpu or cpu\n",
    "model = model.to(device)\n",
    "\n",
    "params_to_update = model.parameters()\n",
    "print(\"Parameters to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = list()\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\", name)\n",
    "else:\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\", name)\n",
    "            \n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcfbc3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 1.23067207922934 Accuracy: 0.4424223407593348\n",
      "val Loss: 1.0744725099840182 Accuracy: 0.5423728813559322\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 1.0817225982598986 Accuracy: 0.5472230938186382\n",
      "val Loss: 0.978732132417783 Accuracy: 0.6054613935969868\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 1.0127376895134899 Accuracy: 0.5889551302165045\n",
      "val Loss: 0.9199248922746733 Accuracy: 0.6177024482109228\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.9506085111236812 Accuracy: 0.615625980545968\n",
      "val Loss: 0.8956581625785756 Accuracy: 0.6224105461393596\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.936298864703961 Accuracy: 0.6262943206777534\n",
      "val Loss: 0.867582985685819 Accuracy: 0.6271186440677966\n",
      "\n",
      "Training Complete in 27m 34s\n",
      "Best Validation Acuuracy: 0.6271\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model, hist = train_model(model, dataloaders_dict, criterion, optimizer_ft, epochs = epochs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(1, 1062):\n",
    "#     if 1062%x == 0:\n",
    "#         print(f'rem: {1062%x} --> {x}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_torch_venv",
   "language": "python",
   "name": "tf_torch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
