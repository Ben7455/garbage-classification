{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6658a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "from torchvision.datasets.folder import default_loader, IMG_EXTENSIONS\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import (TensorDataset, \n",
    "                              Dataset, \n",
    "                              Subset,\n",
    "                              random_split,\n",
    "                              DataLoader,\n",
    "                              RandomSampler, \n",
    "                              SequentialSampler, \n",
    "                              )\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from models import initialize_vision_model, initialize_language_model\n",
    "from GarbageUtils import GarbageDataset, split_dataset, GarbageImageFolder, append_value\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e57284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "PyTorch Version:  1.13.1+cu117\n",
      "Torchvision Version:  0.14.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(f'Device: {device}')\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae7957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "vision_model_name = \"inception\"\n",
    "language_model_name = \"bert-base-uncased\"\n",
    "num_classes = 4\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "feature_extract = True\n",
    "\n",
    "#Note: this won't work with efficientnet, can't multiply the matrices.\n",
    "out_features = 2052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0352ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if language_model_name == \"bert-base-uncased\":\n",
    "    out_features = 2052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475fee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing InceptionV3 with weights=Inception_V3_Weights.DEFAULT...\n",
      "Input size = 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Bert-Base-Uncased...\n"
     ]
    }
   ],
   "source": [
    "vision_model, input_size, vision_out_features = initialize_vision_model(vision_model_name, num_classes, feature_extract, \n",
    "                                                                        use_pretrained=True)\n",
    "language_model, tokenizer = initialize_language_model(language_model_name, num_classes, multimodal=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44fb3f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): InceptionAux(\n",
       "    (conv0): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): BasicConv2d(\n",
       "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08e0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd091b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = GarbageImageFolder(data_dir, is_valid_file=validate_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff24312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset GarbageImageFolder\n",
       "    Number of datapoints: 5312\n",
       "    Root location: ./data"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d040c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black', 'blue', 'green', 'other']\n",
      "Num of Classes: 4\n"
     ]
    }
   ],
   "source": [
    "classes =  image_dataset.classes\n",
    "num_classes = len(classes)\n",
    "print(classes)\n",
    "print(f'Num of Classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d92b7c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'black': 0, 'blue': 1, 'green': 2, 'other': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a38ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<PIL.Image.Image image mode=RGB size=800x800 at 0x22DDCDE8100>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=800x800 at 0x22DEA0008B0>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=800x800 at 0x22DEA000C10>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=800x800 at 0x22DEA000D90>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=800x800 at 0x22DEA000AC0>, 0),\n",
       " (<PIL.Image.Image image mode=RGB size=1734x1301 at 0x22DEA000760>, 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = slice(-3, -1)\n",
    "image_dataset[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9180cfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<PIL.Image.Image image mode=RGB size=1155x1600 at 0x22D8011ACA0>, 3),\n",
       " (<PIL.Image.Image image mode=RGB size=2615x3044 at 0x22D8011ACD0>, 3)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dataset[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3092235b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = split_dataset(image_dataset.imgs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e75ad41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train set size: 3187\n",
      "Val set size: 1062\n",
      "Test set size: 1062\n"
     ]
    }
   ],
   "source": [
    "def get_dataloaders(input_size, train_set, val_set, test_set):\n",
    "    from torchvision import datasets, transforms\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    train_set = GarbageDataset(train_set, is_subset=False, transform=data_transforms['train'])\n",
    "    val_set = GarbageDataset(val_set,  is_subset=False, transform=data_transforms['val'])\n",
    "    test_set = GarbageDataset(test_set,  is_subset=False, transform=data_transforms['val'])\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    print(f'Train set size: {len(train_set)}')\n",
    "    print(f'Val set size: {len(val_set)}')\n",
    "    print(f'Test set size: {len(test_set)}')\n",
    "    \n",
    "    dataloaders_dict = {\n",
    "        'train': DataLoader(train_set, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True),\n",
    "        'val': DataLoader(val_set, batch_size = batch_size, shuffle=False, num_workers=4, drop_last=True)\n",
    "    }\n",
    "    \n",
    "    test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True)\n",
    "    \n",
    "#     print(\"Loading Datasets and Initializing DataLoaders...\")\n",
    "    return test_dataloader, dataloaders_dict\n",
    "\n",
    "test_dataloader, dataloaders_dict = get_dataloaders(input_size, train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f6eb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalGarbageModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes, text_module, vision_module, text_module_name, vision_module_name,\n",
    "                 out_features_combined, dropout_p=None):\n",
    "        super(MultiModalGarbageModel, self).__init__()\n",
    "        self.text_module = text_module\n",
    "        self.vision_module = vision_module\n",
    "        self.text_module_name = text_module_name\n",
    "        self.vision_module_name = vision_module_name\n",
    "#         self.fc = torch.nn.Linear(out_features_combined, int(out_features_combined/2))\n",
    "#         self.fc2 = torch.nn.Linear(int(out_features_combined/2), int(out_features_combined/4))\n",
    "#         self.fc3 = torch.nn.Linear(int(out_features_combined/4), num_classes)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(out_features_combined, int(out_features_combined/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(out_features_combined/2), num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, vision_data, text_data, attention_mask):\n",
    "        text_out, vision_out = 0, 0\n",
    "        \n",
    "        # get output from vision model\n",
    "        if self.vision_module_name == \"inception\":\n",
    "            self.vision_module.aux_logits = False\n",
    "            vision_out = self.vision_module(vision_data)\n",
    "        else:\n",
    "            vision_out = self.vision_module(vision_data)\n",
    "\n",
    "        # get output from text model    \n",
    "        if self.text_module_name == \"bert-base-uncased\":\n",
    "            out = self.text_module(text_data, attention_mask)\n",
    "            text_out = out[0]\n",
    "        else:\n",
    "            text_out = self.text_module(text_data, attention_mask)\n",
    "            \n",
    "        combined = torch.cat((vision_out, text_out), dim=1)\n",
    "        combined = combined.view(combined.size(0), -1)\n",
    "        logits = self.linear_relu_stack(combined)\n",
    "#         combined = self.fc(combined)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9f66601",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalGarbageModel(num_classes, language_model, vision_model, language_model_name,\n",
    "                                          vision_model_name, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c65b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c661f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, epochs=25, is_inception=False, result_dict=None):\n",
    "    since = time.time()\n",
    "    \n",
    "    val_acc_history = list()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "#         print(f'Epoch {epoch+1}/{epochs}')\n",
    "#         print(\"-\" * 10)\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            #Iterate over the data # image_file, label, input_ids, attention_mask, file_name\n",
    "            for inputs, labels, in_ids, att_mask in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                in_ids = in_ids.to(device)\n",
    "                att_mask = att_mask.to(device)\n",
    "                \n",
    "                optimizer.zero_grad() # to zero the parameter gradients\n",
    "                \n",
    "                # forward, track history if only in train mode\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs, in_ids, att_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "#                     if is_inception and phase == \"train\":\n",
    "#                         outputs, aux_outputs = model(inputs)\n",
    "#                         loss1 = criterion(outputs, labels)\n",
    "#                         loss2 = criterion(aux_outputs, labels)\n",
    "#                         loss = loss1 + 0.4*loss2\n",
    "#                     else:\n",
    "#                         outputs = model(inputs)\n",
    "#                         loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # backward, optimize only if in train mode\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "#             print(f'{phase} Loss: {epoch_loss} Accuracy: {epoch_acc}')\n",
    "            print('-' * 59)\n",
    "            print('| Epoch {:3d}/{:3d} | {} Loss: {:8.3f} | {} Accuracy {:8.3f} |'.format(\n",
    "                epoch, epochs, phase, epoch_loss, phase, epoch_acc))\n",
    "            print('-' * 59)\n",
    "            \n",
    "            if result_dict is not None:\n",
    "                append_value(result_dict, \"Epoch\", epoch)\n",
    "                append_value(result_dict, phase+\" Accuracy\", epoch_acc)\n",
    "                append_value(result_dict, phase+\" Loss\", epoch_loss)\n",
    "            \n",
    "            # deepcopy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training Complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Validation Accuracy: {:.04f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "777bf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, dataloaders, criterion, optimizer, epochs=25, is_inception=False):\n",
    "#     since = time.time()\n",
    "    \n",
    "#     val_acc_history = list()\n",
    "    \n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         print(f'Epoch {epoch}/{epochs - 1}')\n",
    "#         print(\"-\" * 10)\n",
    "        \n",
    "#         for phase in [\"train\", \"val\"]:\n",
    "#             if phase == \"train\":\n",
    "#                 model.train()\n",
    "#             else:\n",
    "#                 model.eval()\n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "            \n",
    "#             #Iterate over the data # image_file, label, input_ids, attention_mask, file_name\n",
    "#             for inputs, labels, _, _ in dataloaders[phase]:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "                \n",
    "#                 optimizer.zero_grad() # to zero the parameter gradients\n",
    "                \n",
    "#                 # forward, track history if only in train mode\n",
    "#                 with torch.set_grad_enabled(phase == \"train\"):\n",
    "#                     if is_inception and phase == \"train\":\n",
    "#                         outputs, aux_outputs = model(inputs)\n",
    "#                         loss1 = criterion(outputs, labels)\n",
    "#                         loss2 = criterion(aux_outputs, labels)\n",
    "#                         loss = loss1 + 0.4*loss2\n",
    "#                     else:\n",
    "#                         outputs = model(inputs)\n",
    "#                         loss = criterion(outputs, labels)\n",
    "                    \n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "#                     # backward, optimize only if in train mode\n",
    "#                     if phase == \"train\":\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "                        \n",
    "#                 # statistics\n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "#             epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "#             print(f'{phase} Loss: {epoch_loss} Accuracy: {epoch_acc}')\n",
    "            \n",
    "#             # deepcopy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#             if phase == 'val':\n",
    "#                 val_acc_history.append(epoch_acc)\n",
    "#         print()\n",
    "        \n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training Complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best Validation Acuuracy: {:.04f}'.format(best_acc))\n",
    "    \n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    \n",
    "#     return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87699025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to learn:\n",
      "\t text_module.bert.embeddings.word_embeddings.weight\n",
      "\t text_module.bert.embeddings.position_embeddings.weight\n",
      "\t text_module.bert.embeddings.token_type_embeddings.weight\n",
      "\t text_module.bert.embeddings.LayerNorm.weight\n",
      "\t text_module.bert.embeddings.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.0.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.0.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.1.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.1.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.2.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.2.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.3.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.3.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.4.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.4.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.5.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.5.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.6.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.6.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.7.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.7.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.8.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.8.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.9.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.9.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.10.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.10.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.self.query.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.self.query.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.self.key.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.self.key.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.self.value.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.self.value.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\t text_module.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\t text_module.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\t text_module.bert.encoder.layer.11.output.dense.weight\n",
      "\t text_module.bert.encoder.layer.11.output.dense.bias\n",
      "\t text_module.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\t text_module.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\t text_module.bert.pooler.dense.weight\n",
      "\t text_module.bert.pooler.dense.bias\n",
      "\t text_module.classifier.weight\n",
      "\t text_module.classifier.bias\n",
      "\t vision_module.AuxLogits.fc.weight\n",
      "\t vision_module.AuxLogits.fc.bias\n",
      "\t linear_relu_stack.0.weight\n",
      "\t linear_relu_stack.0.bias\n",
      "\t linear_relu_stack.2.weight\n",
      "\t linear_relu_stack.2.bias\n"
     ]
    }
   ],
   "source": [
    "# send model to device: gpu or cpu\n",
    "model = model.to(device)\n",
    "\n",
    "params_to_update = model.parameters()\n",
    "print(\"Parameters to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = list()\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\", name)\n",
    "else:\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\", name)\n",
    "            \n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfbc3ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| Epoch   1/100 | train Loss:    1.235 | train Accuracy    0.459 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch   1/100 | val Loss:    1.125 | val Accuracy    0.521 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch   2/100 | train Loss:    1.084 | train Accuracy    0.551 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch   2/100 | val Loss:    0.971 | val Accuracy    0.628 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch   3/100 | train Loss:    0.980 | train Accuracy    0.597 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch   3/100 | val Loss:    0.894 | val Accuracy    0.675 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch   4/100 | train Loss:    0.935 | train Accuracy    0.617 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch   4/100 | val Loss:    0.873 | val Accuracy    0.646 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch   5/100 | train Loss:    0.905 | train Accuracy    0.638 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch   5/100 | val Loss:    0.790 | val Accuracy    0.696 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch   6/100 | train Loss:    0.812 | train Accuracy    0.685 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch   6/100 | val Loss:    0.786 | val Accuracy    0.708 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch   7/100 | train Loss:    0.735 | train Accuracy    0.715 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch   7/100 | val Loss:    0.788 | val Accuracy    0.718 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch   8/100 | train Loss:    0.734 | train Accuracy    0.723 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch   8/100 | val Loss:    0.750 | val Accuracy    0.731 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch   9/100 | train Loss:    0.677 | train Accuracy    0.749 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch   9/100 | val Loss:    0.629 | val Accuracy    0.782 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  10/100 | train Loss:    0.630 | train Accuracy    0.760 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  10/100 | val Loss:    0.627 | val Accuracy    0.782 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  11/100 | train Loss:    0.602 | train Accuracy    0.770 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  11/100 | val Loss:    0.664 | val Accuracy    0.754 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  12/100 | train Loss:    0.644 | train Accuracy    0.760 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  12/100 | val Loss:    0.601 | val Accuracy    0.770 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  13/100 | train Loss:    0.547 | train Accuracy    0.790 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  13/100 | val Loss:    0.560 | val Accuracy    0.798 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  14/100 | train Loss:    0.513 | train Accuracy    0.808 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  14/100 | val Loss:    0.539 | val Accuracy    0.801 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  15/100 | train Loss:    0.480 | train Accuracy    0.815 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  15/100 | val Loss:    0.537 | val Accuracy    0.800 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  16/100 | train Loss:    0.433 | train Accuracy    0.841 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  16/100 | val Loss:    0.527 | val Accuracy    0.801 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  17/100 | train Loss:    0.423 | train Accuracy    0.846 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  17/100 | val Loss:    0.575 | val Accuracy    0.782 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  18/100 | train Loss:    0.388 | train Accuracy    0.856 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  18/100 | val Loss:    0.514 | val Accuracy    0.815 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  19/100 | train Loss:    0.357 | train Accuracy    0.870 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  19/100 | val Loss:    0.620 | val Accuracy    0.812 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  20/100 | train Loss:    0.335 | train Accuracy    0.876 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  20/100 | val Loss:    0.552 | val Accuracy    0.808 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  21/100 | train Loss:    0.326 | train Accuracy    0.883 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  21/100 | val Loss:    0.560 | val Accuracy    0.827 |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| Epoch  22/100 | train Loss:    0.278 | train Accuracy    0.907 |\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| Epoch  22/100 | val Loss:    0.607 | val Accuracy    0.814 |\n",
      "-----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model, hist = train_model(model, dataloaders_dict, criterion, optimizer_ft, epochs = 100, result_dict=result_dict,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in range(1, 1062):\n",
    "#     if 1062%x == 0:\n",
    "#         print(f'rem: {1062%x} --> {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201191e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_copy = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73dce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf678d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb857fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalGarbageModel(num_classes, language_model, vision_model, language_model_name,\n",
    "                                          vision_model_name, out_features)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_torch_venv",
   "language": "python",
   "name": "tf_torch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
